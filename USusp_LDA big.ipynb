{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "import numpy as np\n",
    "from time import time\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисляем перплексию модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(model, corpus):\n",
    "    corpus_length = 0\n",
    "    log_likelihood = 0\n",
    "    topic_profiles = model.state.get_lambda() / np.sum(model.state.get_lambda(), axis=1)[:, np.newaxis]\n",
    "    for document in corpus:\n",
    "        gamma, _ = model.inference([document])\n",
    "        document_profile = gamma / np.sum(gamma)\n",
    "        for term_id, term_count in document:\n",
    "            corpus_length += term_count\n",
    "            term_probability = np.dot(document_profile, topic_profiles[:, term_id])\n",
    "            log_likelihood += term_count * log(term_probability)\n",
    "    perplexity = np.exp(-log_likelihood / corpus_length)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составляем коллекцию текстов из документа без учёта стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "with open(\"US_proc_l_b.txt\") as f:\n",
    "    text = f.read()\n",
    "    us_texts = text.split(\"\\n\")\n",
    "    stop = ['и', 'а', 'когда', 'но', 'с', 'к', 'чтобы', 'же', 'ли', 'во', 'над', 'ну', 'два',\n",
    "            'вот', 'более', 'потому', 'быть', 'конечно', 'перед', 'да', 'какая', 'после', 'где', 'сам', 'еще', 'ещё', \n",
    "            'куда', 'даже', 'можно', 'раз', 'или', 'чего', 'об', 'зачем', 'по', 'все', 'мой', 'моя', 'моё', \n",
    "            'мои', 'одна', 'одно', 'без', 'как', \n",
    "            'чтоб', 'в','нет', 'разве', 'впрочем', 'на', 'такой', 'будто', 'при', 'ни', 'когда', 'за', \n",
    "            'со', 'у', 'уж', 'три', 'о', 'про', 'из','чем', 'для', 'до', 'под', 'с', 'чуть', 'если', 'ведь',\n",
    "            'один', 'хоть', 'бы', 'уже', 'не', 'быть', 'есть', 'через', 'только', 'от', 'между', 'чтобы', 'ж',\n",
    "            'тоже', '-', 'б', 'ибо','сильвио', 'кольбер', 'джой','граф', 'графиня', 'пчёлы',  'р', 'анна', \n",
    "            'николаевна', 'я', 'ты', 'он', 'она', 'оно', 'мы', 'вы', 'они', 'его', 'ее', 'её', 'ему', 'ей', \n",
    "            'меня', 'ваше', 'сиятельство', 'маша', 'их', 'вам', 'него', 'мне', 'мною', 'ней', \n",
    "            'нее', 'тебя', 'меня', 'ним', 'нас', 'нему', 'нем', 'кольбера', 'мое', 'моё', 'кузька', \n",
    "            'вера', 'мою', 'африканович', 'ермолай', 'нюшка', 'степановна', 'колюша', 'арина', 'иван',\n",
    "            'нам', 'вас', 'г', 'им', 'парменко', 'катерина', 'евстолья', 'сосновка',  'г', 'им', 'кочерга',\n",
    "            'пчела', 'наш', 'ваш', 'пять', 'шесть']\n",
    "    stops = set(stop)\n",
    "    for i in range(len(us_texts)):\n",
    "        us_texts[i]=us_texts[i].split()\n",
    "        us_texts[i] = [w for w in us_texts[i] if not w in stops]\n",
    "    print(len(us_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составляем словарь по коллекции и фильтруем его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Dictionary(3619 unique tokens: ['особый', 'зелень', 'сени', 'стая', 'седло']...)\n",
      "Filtered: Dictionary(1183 unique tokens: ['особый', 'приходиться', 'оба', 'свинцовый', 'вода']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(us_texts)\n",
    "print ('Original: {}'.format(dictionary))\n",
    "dictionary.filter_extremes(no_below = 2, no_above = 0.5, keep_n=None)\n",
    "dictionary.save('ap3.dict')\n",
    "print ('Filtered: {}'.format(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in us_texts]\n",
    "corpora.MmCorpus.serialize('ap3.mm', corpus) # store on disc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель, выводим её перплексию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation time: 0.004189014434814453\n",
      "Perplexity: 462.10309341059104\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "model = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=15, chunksize=15, update_every=1, passes=2)\n",
    "print ('Evaluation time: {}'.format((time()-start) / 60))\n",
    "print ('Perplexity: {}'.format(perplexity(model, corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим на получившиеся темы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 1 top 10:  взять, точно, зверок, который, обыкновенно, крыльцо, лошадь, этот, сорок, глаз\n",
      "topic 2 top 10:  домой, очевидно, хотеть, спрашивать, кто, там, это, себя, списывать, танцевать\n",
      "topic 3 top 10:  это, сквозь, то, оставаться, нельзя, город, тот, дело, так, год\n",
      "topic 4 top 10:  гора, мочь, лететь, забор, служба, какой, отдаление, баба, пойти, час\n",
      "topic 5 top 10:  свой, кирпич, жена, ворота, <question-mark>, это, столб, открывать, казаться, вокруг\n",
      "topic 6 top 10:  ребенок, который, любить, редко, должный, ко, вместе, отдавать, свой, просьба\n",
      "topic 7 top 10:  окно, знать, выламывать, дом, последний, стекло, колонна, фасад, что, девочка\n",
      "topic 8 top 10:  <question-mark>, что, <exclamation-mark>, говорить, теперь, <ellipsis>, свой, сказать, тут, седой\n",
      "topic 9 top 10:  время, это, чувствовать, становиться, замечать, вода, очень, этот, девка, однако\n",
      "topic 10 top 10:  потом, мерин, девка, забывать, надевать, пармен, стыдно, обед, сперва, запирать\n",
      "topic 11 top 10:  ночь, час, приходить, утро, что, глубокий, погасать, перевал, день, делать\n",
      "topic 12 top 10:  дым, стоять, самовар, старуха, лицо, золотой, самый, ружье, нога, всякий\n",
      "topic 13 top 10:  дом, земля, улица, густой, спать, тогда, пока, долго, нога, свет\n",
      "topic 14 top 10:  рама, человек, гудеть, лес, никогда, снег, хотя, ничто, долина, <ellipsis>\n",
      "topic 15 top 10:  туман, место, другой, становиться, идти, воздух, солнце, скоро, вечер, белый\n"
     ]
    }
   ],
   "source": [
    "for topic in range(0, 15):\n",
    "    words = [i[0] for i in model.show_topic(topic, topn = 10)]\n",
    "    print(\"topic\", topic+1, \"top 10: \", \", \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Записываем в csv top-10 слов каждой темы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top = open('US-LDAtopics.csv', 'w', encoding='utf-8')\n",
    "n=1\n",
    "for topic in range(0, 15):\n",
    "    for i in model.show_topic(topic, topn = 10):\n",
    "        #line = n,',', i[0], ',', i[1], '\\n'\n",
    "        line = str(n), str(i[0]), str(i[1])\n",
    "        line = ','.join(line)+'\\n'\n",
    "        top.write(line)\n",
    "    n+=1\n",
    "top.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также записываем в отдельный файл слова, которые относятся к каждой теме с вероятностью не меньше 0.005:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clear = open ('UST_clear.txt', 'w', encoding='utf-8')\n",
    "for topic in range(0, 15):\n",
    "    for i in model.show_topic(topic, topn = 10):\n",
    "        if i[1]>0.005:\n",
    "            line = str(i[0])+' '\n",
    "            clear.write(line)\n",
    "    clear.write('\\n')\n",
    "clear.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
